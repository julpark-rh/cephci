---
#===============================================================================================
# Tier-level: 2
# Test-Suite: tier-2_cephfs_test-snapshot-clone.yaml
# Conf file : conf/nautilus/cephfs/tier_0_fs.yaml
# Set Up Details:
#  Containerized Deployment
#  Dashboard Status : Enabled
#     Dashboard Username : admin
#     Dashboard Password : p@ssw0rd
#  Grafana Server : true
#     Grafana Username : admin
#     Grafana Passwrod : p@ssw0rd
# Test-Case Covered:
#	CEPH-83573499 Remove the original volume after cloning and verify the data is accessible from cloned volume
#	CEPH-83573501 Create a Cloned Volume using a snapshot
#	CEPH-83573504 Verify the status of cloning operation
#	CEPH-83573502 Interrupt the cloning operation in-between and observe the behavior.
#	CEPH-83573520 Validate the max snapshot that can be created under a root FS sub volume level.
#	              Increase by 50 at a time until it reaches the max limit.
#	CEPH-83573521 Remove a subvolume group by retaining the snapshot : ceph fs subvolume rm <vol_n...
#	CEPH-83573415 Test to validate the cli - ceph fs set <fs_name> allow_new_snaps true
#	CEPH-83573418 Create a Snapshot, reboot the node and rollback the snapshot
#	CEPH-83573420 Try writing the data to snap directory
#   CEPH-83573524	Ensure the subvolume attributes are retained post clone operations
#   CEPH-11319		Create first snap add more data to original then create a second snap.
#                   Rollback 1st snap do data validation. Rollback 2nd snap and do data validation.
#                   Perform cross platform rollback i.e. take snap on kernel mount and perform rollback using fuse mount
#   CEPH-83573255	Try renaming the snapshot directory and rollbackCreate a FS and
#                   create 10 directories and mount them on kernel client and fuse client(5 mounts each)
#                   Add data (~ 100 GB). Create a Snapshot and verify the content in snap directory.
#                   Try modifying the snapshot name.
#   CEPH-83573522	Verify the retained snapshot details with "ceph fs info" command
#===============================================================================================
tests:
  -
    test:
      abort-on-fail: true
      module: install_prereq.py
      name: "install ceph pre-requisites"
  -
    test:
      abort-on-fail: true
      config:
        ansi_config:
          alertmanager_container_image: "registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6"
          ceph_conf_overrides:
            mon:
              mon_allow_pool_delete: true
          ceph_docker_image: rhceph/rhceph-4-rhel8
          ceph_docker_image_tag: latest
          ceph_docker_registry: registry.redhat.io
          ceph_origin: distro
          ceph_repository: rhcs
          ceph_stable_rh_storage: true
          ceph_test: true
          cephfs_pools:
            - name: "cephfs_data"
            - name: "cephfs_metadata"
          containerized_deployment: true
          copy_admin_key: true
          dashboard_admin_password: p@ssw0rd
          dashboard_admin_user: admin
          dashboard_enabled: true
          grafana_admin_password: p@ssw0rd
          grafana_admin_user: admin
          grafana_container_image: "registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4"
          journal_size: 1024
          node_exporter_container_image: "registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6"
          osd_auto_discovery: false
          osd_scenario: collocated
          prometheus_container_image: "registry.redhat.io/openshift4/ose-prometheus:v4.6"
      desc: "ceph deployment using site.yaml(container) playbook"
      destroy-cluster: false
      module: test_ansible.py
      name: "containerized ceph ansible"
  - test:
      name: Clone_status
      module: snapshot_clone.clone_status.py
      polarion-id: CEPH-83573501
      desc: Checks the clone status and states of the clone process
      abort-on-fail: false
  - test:
      name: Clone_cancel_status
      module: snapshot_clone.clone_cancel_status.py
      polarion-id: CEPH-83573502
      desc: Checks the clone status and states of the clone process
      abort-on-fail: false
  - test:
      name: Retain_Snapshots
      module: snapshot_clone.retain_snapshots.py
      polarion-id: CEPH-83573521
      desc: Retains the snapshots after deletig the subvolume
      abort-on-fail: false
  - test:
      name: snapshot_flag
      module: snapshot_clone.snapshot_flag.py
      polarion-id: CEPH-83573415
      desc: Test to validate the cli - ceph fs set <fs_name> allow_new_snaps true
      abort-on-fail: false
  - test:
        name: Remove_Subvolume_clone
        module: snapshot_clone.clone_remove_subvol.py
        polarion-id: CEPH-83573499
        desc: Clone a subvolume and remove the orginal volume and verify the contents in subvolume
        abort-on-fail: false
  - test:
      name: Test Max Snapshot limit
      module: snapshot_clone.max_snapshot_limit.py
      polarion-id: CEPH-83573520
      desc: Validate the max snapshot that can be created under a root FS sub volume level.Increase by 50 at a time until it reaches the max limit.
      abort-on-fail: false
  - test:
      name: Snapshot reboot
      module: snapshot_clone.snapshot_reboot.py
      polarion-id: CEPH-83573418
      desc: Create a Snapshot, reboot the node and rollback the snapshot
      abort-on-fail: false
  - test:
      name: Snapshot write
      module: snapshot_clone.snapshot_write.py
      polarion-id: CEPH-83573420
      desc: Try writing the data to snap directory
      abort-on-fail: false
  - test:
      name: Clone_attributes
      module: snapshot_clone.clone_attributes.py
      polarion-id: CEPH-83573524
      desc: Retains the snapshots after deletig the subvolume
      abort-on-fail: false
  - test:
      name: cross_platform_snaps
      module: snapshot_clone.cross_platform_snaps.py
      polarion-id: CEPH-11319
      desc: Clone a subvolume and remove the orginal volume and verify the contents in subvolume
      abort-on-fail: false
  - test:
      name: rename snap directory
      module: snapshot_clone.rename_snap_dir.py
      polarion-id: CEPH-83573255
      desc: Validate the max snapshot that can be created under a root FS sub volume level.Increase by 50 at a time until it reaches the max limit.
      abort-on-fail: false
  - test:
      name: subvolume_info_retain
      module: snapshot_clone.subvolume_info_retain.py
      polarion-id: CEPH-83573522
      desc: Create a Snapshot, reboot the node and rollback the snapshot
      abort-on-fail: false
